# -*- coding: utf-8 -*-
"""Kozhamuratova_Aray_nlp_midterm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17dOhdTJGvYWrMkOllVj489fTSiZs5wLx

**Task 1: Data Preprocessing and Word Embeddings**
Objective: Implement basic preprocessing steps, including tokenization, stopword removal, and lemmatization, and apply word embeddings for representing text data.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/My Drive/nlp/Twitter_Data.csv'
df = pd.read_csv(file_path)
df.head(100)

df.rename(columns={'category': 'label', 'clean_text': 'text'}, inplace=True)
print(df.columns)

df.info()
df['label'].value_counts()

df['label'] = df['label'].map({-1.0: 0, 0.0: 1, 1.0: 2})
print(df)

print(df['label'].unique())

df = df.dropna(subset=['label'])
df = df[df["label"] != 1]
df['label'] = df['label'].astype(int)

print(df['label'].unique())

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")

lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#\w+", "", text)
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words("english")]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

df["clean_text"] = df["text"].apply(preprocess_text)
df.head()

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df["clean_text"])
sequences = tokenizer.texts_to_sequences(df["clean_text"])

max_length = 50
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding="post")

labels = np.array(df["label"])
print(padded_sequences[:5])

import numpy as np

unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.array([0, 2])

class_weights = compute_class_weight("balanced", classes=classes, y=y_train)
class_weights_dict = dict(zip(classes, class_weights))

print("Corrected Class Weights:", class_weights_dict)

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=max_length),
    LSTM(64, return_sequences=True),
    LSTM(64),
    Dropout(0.5),
    Dense(1, activation="sigmoid")
])

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=15, batch_size=32,
    class_weight=class_weights_dict
)

model.summary()

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

sample_texts = ["I love you", "This is terrible thing!", "Worst experience ever!", "Amazing product.", "I hate this."]
sample_sequences = tokenizer.texts_to_sequences(sample_texts)
sample_padded = pad_sequences(sample_sequences, maxlen=max_length)

predictions = model.predict(sample_padded)
print(predictions)

predicted_labels = ["negative" if p > 0.5 else "positive" for p in predictions]

for text, sentiment in zip(sample_texts, predicted_labels):
    print(f"{text} -> {sentiment}")

model.save("/content/drive/My Drive/nlp/sentiment_model_binary.h5")
"""from tensorflow.keras.models import load_model
loaded_model = load_model("/content/drive/My Drive/nlp/sentiment_model_binary.h5")"""

from tensorflow.keras.models import load_model

model = load_model("/content/drive/My Drive/nlp/sentiment_model_binary.h5")

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

from gensim.models import Word2Vec

sentences = [text.split() for text in df["clean_text"]]

word2vec_cbow = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=0)
word2vec_skipgram = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=1)

print("Word2Vec models trained successfully!")

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

words = list(word2vec_cbow.wv.index_to_key)[:50]
vectors = [word2vec_cbow.wv[word] for word in words]

pca = PCA(n_components=2)
reduced_vectors = pca.fit_transform(vectors)

plt.figure(figsize=(10, 6))
plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1])

for i, word in enumerate(words):
    plt.annotate(word, xy=(reduced_vectors[i, 0], reduced_vectors[i, 1]))

plt.title("Word2Vec Word Embeddings (PCA)")
plt.show()

from gensim.models import Word2Vec

sentences = [text.split() for text in df["clean_text"]]
word2vec_cbow = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=0)
word2vec_skipgram = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=1)

import gensim.downloader as api

glove_model = api.load("glove-wiki-gigaword-100")
glove_vector = glove_model["happy"]
print(glove_vector)

from gensim.models import FastText

fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=2)

word = "happy"

print("Word2Vec (CBOW) Similar Words:")
print(word2vec_cbow.wv.most_similar(word))

print("GloVe Similar Words:")
print(glove_model.most_similar(word))

print("FastText Similar Words:")
print(fasttext_model.wv.most_similar(word))

"""**Task 2: Building Recurrent Neural Networks for Sentiment Analysis**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

print("Training Data Shape:", X_train.shape)
print("Testing Data Shape:", X_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout

print("X_train shape:", X_train.shape)

model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1]),
    SimpleRNN(64, return_sequences=True),
    SimpleRNN(64),
    Dropout(0.5),
    Dense(1, activation="sigmoid")
])

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

model.build(input_shape=(None, X_train.shape[1]))

model.summary()

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=10, batch_size=32
)

import matplotlib.pyplot as plt

plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Val Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Model Accuracy")
plt.show()

plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Model Loss")
plt.show()

import tensorflow.keras.backend as K
import numpy as np
import tensorflow as tf

def get_gradients(model, X_sample, y_sample):
    with tf.GradientTape() as tape:
        tape.watch(X_sample)
        y_pred = model(X_sample, training=True)
        y_sample = tf.reshape(y_sample, (-1, 1))
        loss = tf.keras.losses.binary_crossentropy(y_sample, y_pred)
    gradients = tape.gradient(loss, model.trainable_variables)
    gradients = [tf.convert_to_tensor(g) if isinstance(g, tf.IndexedSlices) else g for g in gradients]

    return [np.mean(K.eval(g)) for g in gradients if g is not None]

X_sample, y_sample = X_test[:10], y_test[:10]

X_sample = tf.convert_to_tensor(X_sample, dtype=tf.float32)
y_sample = tf.convert_to_tensor(y_sample, dtype=tf.float32)

y_sample = tf.reshape(y_sample, (-1, 1))

gradients = get_gradients(model, X_sample, y_sample)

plt.plot(gradients, marker="o")
plt.title("Gradients in Simple RNN Model")
plt.xlabel("Layer")
plt.ylabel("Gradient Magnitude")
plt.show()

"""**Task 3: Implementing LSTM and GRU for Text Classification**"""

import time
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

lstm_model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1]),
    LSTM(64, return_sequences=True),
    LSTM(64),
    Dropout(0.5),
    Dense(1, activation="sigmoid")
])

lstm_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

start_time = time.time()
lstm_history = lstm_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=10, batch_size=32
)
lstm_time = time.time() - start_time

print(f" LSTM Training Time: {lstm_time:.2f} seconds")

from tensorflow.keras.layers import GRU

gru_model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1]),
    GRU(64, return_sequences=True),
    GRU(64),
    Dropout(0.5),
    Dense(1, activation="sigmoid")
])

gru_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

start_time = time.time()
gru_history = gru_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=10, batch_size=32
)
gru_time = time.time() - start_time

print(f"GRU Training Time: {gru_time:.2f} seconds")

import matplotlib.pyplot as plt

plt.plot(lstm_history.history["accuracy"], label="LSTM Train Accuracy")
plt.plot(lstm_history.history["val_accuracy"], label="LSTM Val Accuracy")
plt.plot(gru_history.history["accuracy"], label="GRU Train Accuracy")
plt.plot(gru_history.history["val_accuracy"], label="GRU Val Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("LSTM vs. GRU Accuracy")
plt.show()

plt.plot(lstm_history.history["loss"], label="LSTM Train Loss")
plt.plot(lstm_history.history["val_loss"], label="LSTM Val Loss")
plt.plot(gru_history.history["loss"], label="GRU Train Loss")
plt.plot(gru_history.history["val_loss"], label="GRU Val Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("LSTM vs. GRU Loss")
plt.show()

lstm_final_acc = lstm_history.history["val_accuracy"][-1]
gru_final_acc = gru_history.history["val_accuracy"][-1]
lstm_final_loss = lstm_history.history["val_loss"][-1]
gru_final_loss = gru_history.history["val_loss"][-1]

print("Performance Comparison:")
print(f"LSTM - Accuracy: {lstm_final_acc:.4f}, Loss: {lstm_final_loss:.4f}, Training Time: {lstm_time:.2f} sec")
print(f"GRU  - Accuracy: {gru_final_acc:.4f}, Loss: {gru_final_loss:.4f}, Training Time: {gru_time:.2f} sec")

if lstm_final_acc > gru_final_acc:
    print("\n LSTM performs better!")
else:
    print("\n GRU performs better!")

"""**Task 4: Text Generation with LSTM**"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
import matplotlib.pyplot as plt

path_to_file = tf.keras.utils.get_file("alice.txt",
                                       "https://www.gutenberg.org/files/11/11-0.txt")

with open(path_to_file, "r", encoding="utf-8") as f:
    text = f.read()

print("Dataset Loaded!")
print(f"Total Characters: {len(text)}")
print(f"Sample Text:\n{text[:500]}")

vocab = sorted(set(text))
char_to_index = {char: i for i, char in enumerate(vocab)}
index_to_char = {i: char for i, char in enumerate(vocab)}

text_as_int = np.array([char_to_index[char] for char in text])

print(f"Unique Characters: {len(vocab)}")
print(f"Sample Mapping: {char_to_index}")

seq_length = 100

char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text[-1]

dataset = sequences.map(split_input_target)
dataset = dataset.shuffle(10000).batch(64, drop_remainder=True)

print("Training Data Prepared!")

model = Sequential([
    Embedding(len(vocab), 256, input_length=seq_length),
    LSTM(1024, return_sequences=False),
    Dense(len(vocab), activation="softmax")
])

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

dummy_input = np.random.randint(0, len(vocab), (1, seq_length))
model.predict(dummy_input)

model.summary()

EPOCHS = 30
history = model.fit(dataset, epochs=EPOCHS)

model.save("lstm_text_generator30.h5")
print("Model saved successfully!")

plt.plot(history.history['loss'], label="Train Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss Over Time")
plt.legend()
plt.show()

import tensorflow as tf
import numpy as np

def generate_text(model, start_string, num_generate=500):
    input_eval = [char_to_index[s] for s in start_string]
    input_eval = tf.expand_dims(input_eval, 0)

    text_generated = []
    temperature = 0.3

    for _ in range(num_generate):
        predictions = model(input_eval)
        predictions = tf.squeeze(predictions, 0)

        predictions = tf.expand_dims(predictions, 0)

        predictions = predictions / temperature
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()

        input_eval = tf.expand_dims([predicted_id], 0)
        text_generated.append(index_to_char[predicted_id])

    return start_string + "".join(text_generated)

seed_text = "Alice was beginning to get very tired"
generated_text = generate_text(model, seed_text)
print("\nGenerated Text:\n", generated_text)

"""**Task 5: Improving Performance with Bidirectional LSTM**"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

import numpy as np

embedding_index = {}
with open("glove.6B.100d.txt", encoding="utf-8") as f:
    for line in f:
        values = line.split()
        word = values[0]
        embedding_index[word] = np.array(values[1:], dtype="float32")

print("Loaded GloVe embeddings:", len(embedding_index))

embedding_dim = 100
embedding_matrix = np.zeros((len(vocab), embedding_dim))

for word, i in char_to_index.items():
    vector = embedding_index.get(word)
    if vector is not None:
        embedding_matrix[i] = vector

from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential

model = Sequential([
    Embedding(len(vocab), embedding_dim, weights=[embedding_matrix], trainable=False, input_length=seq_length),
    LSTM(256, return_sequences=False),
    Dense(len(vocab), activation="softmax")
])

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.build(input_shape=(None, seq_length))
model.summary()

EPOCHS = 10
history = model.fit(dataset, epochs=EPOCHS)

import tensorflow as tf

def generate_text_top_k(model, start_string, num_generate=500, top_k=5):
    input_eval = [char_to_index[s] for s in start_string]
    input_eval = tf.expand_dims(input_eval, 0)
    text_generated = []

    for _ in range(num_generate):
        predictions = model(input_eval)
        predictions = tf.squeeze(predictions, 0)

        top_k_values, top_k_indices = tf.math.top_k(predictions, k=top_k)
        predicted_id = np.random.choice(top_k_indices.numpy().flatten())

        input_eval = tf.expand_dims([predicted_id], 0)
        text_generated.append(index_to_char[predicted_id])

    return start_string + "".join(text_generated)

seed_text = "Alice was beginning to get very tired"
generated_text = generate_text_top_k(model, seed_text)
print("\nGenerated Text:\n", generated_text)